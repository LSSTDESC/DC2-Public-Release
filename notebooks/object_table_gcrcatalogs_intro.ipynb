{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rubin LSST DESC DC2: Accessing Object Table with GCRCatalogs\n",
    "\n",
    "**Authors**: Yao-Yuan Mao (@yymao), Francois Lanusse (@EiffL), Javier Sanchez (@fjaviersanchez), Michael Wood-Vasey (@wmwv)\n",
    "\n",
    "This notebook will illustrate the basics of accessing the Object Table, which contains the detected objects at the coadd level using GCRCatalogs.\n",
    "\n",
    "**Learning objectives**: After going through this notebook, you should be able to:\n",
    "  1. Load and efficiently access a DC2 object table with the GCRCatalogs\n",
    "  2. Understand and have references for the object table schema\n",
    "  3. Apply cuts to the catalog using GCRQuery\n",
    "  4. Have an example of quality cuts and simple star/galaxy separation cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages\n",
    "\n",
    "```bash\n",
    "conda install lsstdesc-gcr-catalogs -c conda-forge\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/global/homes/y/yymao/desc/generic-catalog-reader/\")\n",
    "sys.path.insert(0, \"/global/homes/y/yymao/desc/gcr-catalogs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GCRCatalogs\n",
    "from GCRCatalogs import GCRQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access object table with GCRCatalogs\n",
    "\n",
    "The [GCRCatalogs](https://github.com/LSSTDESC/gcr-catalogs) package is a DESC project which aims at gathering in one convenient location various simulation/data catalogs made available to the collaboration.  \n",
    "In this section, we illustrate how to use this tool to access the object catalogs from DC2 Run2.2i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cat = GCRCatalogs.register.load_catalog_from_config_dict({ \n",
    "    \"subclass_name\": \"dc2_object.DC2ObjectParquetCatalog\",\n",
    "    \"base_dir\": \"/global/cfs/cdirs/lsst/gsharing/dc2_public/object_dpdd\",\n",
    "    \"filename_pattern\": \".+\\.parquet$\",\n",
    "    \"is_dpdd\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Table schema\n",
    "\n",
    "To see the \"columns\" (sometimes called \"quantities\") available in the object table, you can call `list_all_quantities()`.\n",
    "The returned list of `list_all_quantities` is not sorted. Sorting it would make it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(obj_cat.list_all_quantities())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definitions, units, and types of these fields is documented in the data release note (Table 1).\n",
    "As explained in the note, the values exposed here are not the native columns produced by the Data Management stack.\n",
    "Instead, this schema strives to mimic the schema of the LSST Data Products Definition Document [DPDD](http://ls.st/dpdd), \n",
    "which is an effort made by the Rubin to standardize the format of the official Data Release Products (DRP).\n",
    "\n",
    "If the data release note is many clicks away, you can also check the definitions, units, and types of these columns\n",
    "using `get_quantity_info()`. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cat.get_quantity_info(\"Iyy_pixel_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the catalog includes:\n",
    "* Positions\n",
    "* Fluxes and magnitudes (PSF and [CModel](https://www.sdss.org/dr12/algorithms/magnitudes/#cmodel))\n",
    "* Adaptive second moments (using [GalSim's HSM](http://galsim-developers.github.io/GalSim/namespacegalsim_1_1hsm.html))\n",
    "* Quality flags: e.g, does the source have any interpolated pixels? Has any of the measurement algorithms returned an error?\n",
    "* Other useful quantities: `blendedness`, measure of how flux is affected by neighbors: (1 - flux.child/flux.parent) (see 4.9.11 of [Bosch et al. 2018](http://adsabs.harvard.edu/abs/2018PASJ...70S...5B)); `extendedness`, classifies sources as extended or psf-like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing data in an interactive session\n",
    "\n",
    "The DC2 DR6 is a very large data set; loading the full object table into memory is usually not feasible. To access the data in a memory efficient fashion while maintain reasonable performace, we usually need to do one or a combination of the following measures. \n",
    "\n",
    "1. Loading only the columns that you immediately need\n",
    "2. Loading a subset of the sky area that you immediately need, or iterating over sky areas (\"tracts\"; see below)\n",
    "3. Sampling the table at random if your science allows\n",
    "\n",
    "Becuase the physical data files are in Parquet format, loading specific columns (1) is efficient and does not require reading in the full table. \n",
    "The physical data files are also partitioned in \"tracts\", each of which corresponds to a region of the sky (see Figure 1 of the release note); hence, loading only a subset of \"tracts\" or iterate over tracts (2) will also make the memory footprint much smaller. \n",
    "Finally, subsampling the table at random can make your analysis calculation faster and use less memory, but it does not save much in term of I/O as all the rows need to be read in before subsampling.\n",
    "\n",
    "\n",
    "#### If you use GCRCatalogs to access the data\n",
    "\n",
    "You will use `get_quantities` to retrive the data, and there are several convient, high-level features that help you achieve the three measures described above. When calling `get_quantities`, you can\n",
    "\n",
    "1. Specify the columns to load\n",
    "2. Add `tract_filter(tracts)` to `native_filters` to select tracts, and use `return_iterator` to iterate over tracts\n",
    "3. Add `sample_filter(frac)` to `filters` to sample the data frame at random\n",
    "\n",
    "We will demostrate all these features in this notebook.\n",
    "\n",
    "\n",
    "#### If you use pandas to read the data files directly\n",
    "\n",
    "You can achieve the three measures described above by:\n",
    "\n",
    "1. Specifying the `columns` argument when calling `pandas.read_parquet`\n",
    "2. Reading in one file (i.e., one tract) at a time, and only reading in the tracts you need\n",
    "3. Using `pandas.DataFrame.sample` to sample the data frame at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GCRCatalogs' `get_quantities`\n",
    "\n",
    "Let's first define two convenience functions `tract_filter` and `sample_filter` that return `GCRQuery` objects. \n",
    "The `GCRQuery`s returned by `tract_filter` can be used in `native_filters` to select on tracts, \n",
    "while those returned by `sample_filter` can be used in `filters` to  randomly sample the object table.\n",
    "\n",
    "These two convenience functions use some advanced features of `GCRQuery`. \n",
    "For the purpose of this tutorial, you can just consider them implementation details and there is no need to fully understand them. \n",
    "`GCRQuery` is a subclass of `easyquery.Query`;\n",
    "for more detial on how to customize GCRQuery, see https://github.com/yymao/easyquery#usage. \n",
    "We will also demostarte some basic usage of `GCRQuery` in the later part of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convenience functions `tract_filter` and `sample_filter`\n",
    "\n",
    "def tract_filter(tracts, tract_high=None):\n",
    "    \"\"\"\n",
    "    Returns a GCRQuery object to be used in the `native_filters` argument of get_quantities(),\n",
    "    to select only the tracts in *tracts* (a list of integers). \n",
    "    \n",
    "    If *tracts* is a single integer, select only that tract.\n",
    "    If *tracts* and *tract_high* are both given as single integers, select [tracts, tract_high).\n",
    "    \"\"\"\n",
    "    if isinstance(tracts, int):\n",
    "        if tract_high is None:\n",
    "            return GCRQuery('tract == {}'.format(tracts)) \n",
    "        elif isinstance(tract_high, int):\n",
    "            return GCRQuery('tract >= {}'.format(tracts), 'tract < {}'.format(tract_high))\n",
    "        raise ValueError(\"When `tracts` is an integer, `tract_high` must be an integer or None.\")\n",
    "    tracts = np.unique(np.asarray(tracts, dtype=np.int))\n",
    "    if not tracts.size:\n",
    "        raise Value(\"Must select at least one tract.\")\n",
    "    return GCRQuery((lambda t, ts=tracts: np.in1d(t, ts, assume_unique=True), \"tract\"))\n",
    "\n",
    "\n",
    "def sample_filter(frac, random_state=None):\n",
    "    \"\"\"\n",
    "    Returns a GCRQuery object to be used in the `filters` argument of get_quantities()\n",
    "    to randomly sample the object catalog by a given fraction (*frac*).\n",
    "    \"\"\"\n",
    "    frac = float(frac)\n",
    "    if frac == 1:\n",
    "        return GCRQuery()\n",
    "    if frac > 1 or frac <= 0:\n",
    "        raise ValueError(\"`frac` must be a float number in (0, 1].\")\n",
    "    if not isinstance(random_state, np.random.RandomState):\n",
    "        random_state = np.random.RandomState(random_state)\n",
    "    seed = random_state.randint(65536)\n",
    "    def _sampler(t, f=frac, s=seed):\n",
    "        l = len(t)\n",
    "        return (np.random.RandomState(t[0] + s).rand(l) < f) if l else np.zeros(0, dtype=np.bool)\n",
    "    return GCRQuery((_sampler, \"tract\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call signature of `get_quantities`\n",
    "\n",
    "```python\n",
    "obj_cat.get_quantities(quantities=[...], filters=[...], native_filters=[...], return_iterator=True/False)\n",
    "```\n",
    "\n",
    "You can supply:\n",
    "* a list of column names of those you want to load to `quantities`. \n",
    "* a list of `GCRQuery`s (including `sample_filter`'s return and other quality cuts (see later part of this notebook)) to `filters`\n",
    "* a list of tract-selecting `GCRQuery`s (i.e., those retured by  `tract_filter`) to `native_filters`\n",
    "* `True` to `return_iterator` if you want to iterate over tracts, or `False` (default) if you want to concatenate data across tracts. \n",
    "\n",
    "`get_quantities` will return (or yield, if `return_iterator=True`) a Python dictionary that contains\n",
    "columns names as keys and corresponding column arrays as values. \n",
    "\n",
    "Let's look at an example! We would like to retrieve the coordinates (ra, dec) of all objects in the tracts 4225, 4226, and 4430, and apply a down-sampling fraction of 1%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = obj_cat.get_quantities(\n",
    "    quantities=['ra', 'dec'],                            # columns we want to load, \n",
    "    filters=[sample_filter(0.01)],                       # down sample at 1%\n",
    "    native_filters=[tract_filter([4225, 4226, 4430])],   # select three tracts\n",
    "    return_iterator=False,                               # concatenate data across tracts \n",
    ")\n",
    "\n",
    "# Plot a 2d histogram of sources \n",
    "fig, ax = plt.subplots(figsize=(10,6), dpi=100)\n",
    "im = ax.hist2d(data['ra'], data['dec'], 100)[-1]\n",
    "ax.set_xlabel('RA [deg]');\n",
    "ax.set_ylabel('Dec [deg]');\n",
    "plt.colorbar(im, ax=ax, label='1% of total number of objects per bin');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Returned data of `get_quantities`\n",
    "\n",
    "As mentioned above, the data returned by the GCR is structured as a native Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can also easily be converted into a [Pandas DataFrame](https://pandas.pydata.org), if you are so inclined ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or an [astropy table](https://docs.astropy.org/en/stable/table/) too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.table\n",
    "\n",
    "table = astropy.table.Table(data)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `return_iterator` when calling `get_quantities`\n",
    "\n",
    "Concatenating data across tracts is convenient but costs more memory. If your analysis allows, a more memory efficent approach would be to reduce the data for one tract, stored the reduced product, and repeat for other tracts. The `return_iterator` option will turn `get_quantities` into a generator function (i.e., can used in a for loop directly, like `range`), making it easy for users to iterate over tracts.\n",
    "\n",
    "In the example below, we will loop over ten tracts that have the same RA range (you can look up tract numbers in Figure 1 of the release note).\n",
    "For each tract, we will load 0.1% of the objects and put them on a scatter plot. We will also print out a message to see where we are in the for loop. When the loop finishes, we should see a beatiful and colorful strip!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "\n",
    "for data in obj_cat.get_quantities(                     # note how we use `get_quantities` directly in a for loop\n",
    "    quantities=['ra', 'dec', 'tract'],                  # columns we want to load, \n",
    "    filters=[sample_filter(0.001)],                     # down sample at 0.1%\n",
    "    native_filters=[tract_filter(3075, 3086)],          # select 10 tracts\n",
    "    return_iterator=True,                               # return an iterator that iterates over tracts\n",
    "):\n",
    "    print(\"Plotting data from tract\", data[\"tract\"][0])  # print out which tract is current being plotted\n",
    "    ax.scatter(data['ra'], data['dec'], s=1, label=str(data[\"tract\"][0]));\n",
    "    # You can put more science here! \n",
    "\n",
    "ax.set_xlabel('RA [deg]');\n",
    "ax.set_ylabel('Dec [deg]');\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"Tract\", handletextpad=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying more filters (quality cuts or selection cuts)\n",
    "\n",
    "So far we have only used the sampler in the `filters` option of `get_quantities`. \n",
    "In a more realistic setting, you likely would want to apply certain quality cuts or selection cuts to your data. \n",
    "You can of course apply these cuts _after_ you read in the data, especially if you want to play with different cut choices. \n",
    "On the other hand, if you already know certain quality cuts or selection cuts that you always want to apply, \n",
    "then applying them at data load time (i.e., when calling `get_quantities`) can save some memory for you! \n",
    "\n",
    "*Note*: If you want to apply cuts after reading in the data, it would be easiler to first convert the returned data \n",
    "(which is a python dictionary) into a pandas data frame or an astropy table (see instructions in the earlier part of this notebook), \n",
    "and then apply the cuts. This way, all columns will be filtered with the same cuts at once. \n",
    "\n",
    "To apply cuts at data load time, you first need to specify your quality cuts or selection cuts using `GCRQuery`, \n",
    "and then add them to the `filters` option of `get_quantities`.\n",
    "You can read more about how to specify the queries [here](https://github.com/yymao/easyquery#usage), \n",
    "but we will show you some common examples below.\n",
    "\n",
    "Each query should be a string that contains a *simple* python expression, in which the variables are column names. \n",
    "The expression should evaluate to a boolean array. \n",
    "\n",
    "Alternatively, a query can also be in the form of `GCRQuery((func, col1, col2, ...))` (note the two pairs of parentheses).\n",
    "In this form, `col1`, `col2`, ... should be column names, and `func(d[col1], d[col2], ...)` should return a boolean array\n",
    "that has the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic query constructions\n",
    "\n",
    "GCRQuery('clean')  # selects `clean` == True  -- note that clean is a boolean arry itself\n",
    "GCRQuery('extendedness == 0')  # selects `extendedness` == 0\n",
    "GCRQuery((np.isfinite, 'mag_r_cModel'))  # selects `mag_r_cModel` is finite\n",
    "GCRQuery((np.isfinite, 'mag_r_cModel'), \"mag_r_cModel < 25\")  # selects `mag_r_cModel` is finite *AND* mag_r_cModel < 25\n",
    "GCRQuery(\"mag_g_cModel - mag_r_cModel < 1\")  # selects `mag_g_cModel` - `mag_r_cModel` < 1\n",
    "\n",
    "\n",
    "# You can name the queries, and do logical operations (AND &, OR |, XOR ^, NOT ~) on them\n",
    "\n",
    "is_extended = GCRQuery('extendedness == 1')\n",
    "clean = GCRQuery('clean')\n",
    "not_clean = ~clean\n",
    "finite_g = GCRQuery((np.isfinite, 'mag_g_cModel'))\n",
    "finite_r = GCRQuery((np.isfinite, 'mag_r_cModel'))\n",
    "bright_g = finite_g & GCRQuery(\"mag_g_cModel < 25\")\n",
    "bright_r = finite_r & GCRQuery(\"mag_r_cModel < 25\")\n",
    "bright_g_or_r = bright_g | bright_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you defined these queries, you can just put them in a list that you supply to the `filters` option of `get_quantities`. \n",
    "\n",
    "All queries in a lists will be joined by *AND*. If `a`, `b`, and `c` are all `GCRQuery` objects, then:\n",
    "\n",
    "- `filters=[a, b, c]` is the same as `filters=[a & b & c]`\n",
    "- `filters=[a, b | c]` means selecting on `a` AND (`b` OR `c`)\n",
    "\n",
    "\n",
    "Specifying `filters` when calling `get_quantities` is similar to slicing the data based on the `GCRQuery` that you supply. \n",
    "For example, the following three methods will all produce the same data frames:\n",
    "\n",
    "```python\n",
    "# method1\n",
    "df = pd.DataFrame(obj_cat.get_quantities([\"ra\", \"dec\"], filters=[GCRQuery(\"mag_g_cModel - mag_r_cModel < 1\")]))\n",
    "\n",
    "# method2\n",
    "df = pd.DataFrame(obj_cat.get_quantities([\"ra\", \"dec\", \"mag_g_cModel\", \"mag_r_cModel\"]))\n",
    "df = df.query(\"mag_g_cModel - mag_r_cModel < 1\")\n",
    "df = df[\"ra\", \"dec\"]\n",
    "\n",
    "# method3\n",
    "df = pd.DataFrame(obj_cat.get_quantities([\"ra\", \"dec\", \"mag_g_cModel\", \"mag_r_cModel\"]))\n",
    "df = df[df[\"mag_g_cModel\"] - df[\"mag_r_cModel\"] < 1]\n",
    "df = df[\"ra\", \"dec\"]\n",
    "```\n",
    "\n",
    "Note that when using `filters`, you don't need to add the quantities, that are only used in the filter queries to the `quantities` argument. \n",
    "If you do want to keep those quantities, remember to add them to `quantities`  too. \n",
    "For those who are familiar with SQL, the behavior here is analogous to `SELECT <quantities> WHERE <filters>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to store the reduced histogram during the iteration \n",
    "color_bins = np.linspace(-2, 3, 101)\n",
    "hist = np.zeros(len(color_bins)-1, dtype=np.int)\n",
    "\n",
    "# iterate over tract using get_quantities(..., return_iterator=True)\n",
    "for data in obj_cat.get_quantities(\n",
    "    quantities=['mag_g_cModel', 'mag_r_cModel'],                            # columns we want to load, \n",
    "    filters=[sample_filter(0.1), clean, is_extended, bright_g | bright_r],  # filters we want, including the sampler\n",
    "    native_filters=[tract_filter([4225, 4226, 4430])],                      # select three tracts\n",
    "    return_iterator=True,                                                   # return an iterator\n",
    "):\n",
    "    gr = data[\"mag_g_cModel\"] - data[\"mag_r_cModel\"]\n",
    "    hist += np.histogram(gr, color_bins)[0]  # accumulate the histogram\n",
    "\n",
    "# Plot a 1d histogram of colors \n",
    "fig, ax = plt.subplots(figsize=(8,5), dpi=100)\n",
    "ax.bar(color_bins[:-1], hist, width=np.ediff1d(color_bins))\n",
    "ax.set_xlabel(\"Galaxy color $g-r$ [mag]\");\n",
    "ax.set_ylabel(\"10% of counts per bin\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Science example: Star/galaxy separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we have `extendedness` as a tool for star/galaxy classification. An object is considered extended if the the difference between the `PSF` magnitude and the [`CModel` magnitude](https://www.sdss.org/dr12/algorithms/magnitudes/#cmodel) is beyond certain threshold (0.0164). To know more about this see [Bosch et al. 2018](http://adsabs.harvard.edu/abs/2018PASJ...70S...5B) section 4.9.10.\n",
    "\n",
    "Let's take a look at the colors of these objects that we think are stars. We will do this on only one tract to save run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_cuts = [\n",
    "    GCRQuery('clean'), # The source has no flagged pixels (interpolated, saturated, edge, clipped...) \n",
    "                       # and was not skipped by the deblender\n",
    "    GCRQuery('extendedness == 0'),\n",
    "    GCRQuery((np.isfinite, 'mag_g_cModel')),\n",
    "    GCRQuery((np.isfinite, 'mag_r_cModel')),\n",
    "    GCRQuery((np.isfinite, 'mag_i_cModel')),\n",
    "]\n",
    "\n",
    "# columns we need -- note that you don't need to include the columns used in your cuts if you don't need them\n",
    "# for purposes other than the cuts!\n",
    "columns = ['mag_g_cModel', 'mag_r_cModel', 'mag_i_cModel']\n",
    "\n",
    "# loading the data\n",
    "data = obj_cat.get_quantities(\n",
    "    quantities=columns, \n",
    "    filters=star_cuts, \n",
    "    native_filters=[tract_filter(3830)]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,6), dpi=100)\n",
    "im = ax.hexbin(\n",
    "    data['mag_g_cModel']-data['mag_r_cModel'],\n",
    "    data['mag_r_cModel']-data['mag_i_cModel'],\n",
    "    bins='log', \n",
    "    extent=[-1,2,-1,2]\n",
    ")\n",
    "ax.set_xlabel('$g-r$ [mag]');\n",
    "ax.set_ylabel('$r-i$ [mag]');\n",
    "plt.colorbar(im, ax=ax, label='log(Number of objects)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-python-bleed",
   "language": "python",
   "name": "desc-python-bleed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
